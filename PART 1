import requests
import csv
from bs4 import BeautifulSoup

def scrape_product_listings(url):
       response = requests.get(url)

       soup = BeautifulSoup(response.text, 'html.parser')

       listings = soup.find_all('div', class_='sg-col-inner')

      for listing in listings:
	name_elem = listing.find('span', class_=‘a-size-medium')
	price_elem = listing.find('span', class_=‘a-price-whole')
	rating_elem = listing.find('span', class_=‘a-icon-alt')
	reviews_elem = listing.find('span', class_='a-size-base')

            if name_elem and price_elem and rating_elem and reviews_elem:
            	name = name_elem.text.strip()
            	price = price_elem.text.strip()
            	rating = rating_elem.text.strip().split(' ')[0]
            	reviews = reviews_elem.text.strip().replace(',', '')

                        url_elem = listing.find('a', class_='a-link-normal')
            	if url_elem:
                		product_url = 'https://www.amazon.in' + url_elem['href']
            	else:
                		product_url = ''

                       yield {
                'Product URL': product_url,
                'Product Name': name,
                'Product Price': price,
                'Rating': rating,
                'Number of Reviews': reviews
            }

csv_file = open('scraped_products.csv', 'w', newline='', encoding='utf-8')
csv_writer = csv.DictWriter(csv_file, fieldnames=['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])
csv_writer.writeheader()

base_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_'
num_pages = 20

for page in range(1, num_pages+1):
    url = base_url + str(page)
    product_listings = scrape_product_listings(url)

    for product in product_listings:
        csv_writer.writerow(product)

csv_file.close()
